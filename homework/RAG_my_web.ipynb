{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18dd1b5-bb8e-4b56-951c-60ada842e056",
   "metadata": {},
   "source": [
    "# 第四周作业 RAG - Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "911dc815-8641-48ca-b76d-0c4282c7583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8835de9-9e7b-4502-8e60-215556409d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f996e2-ddc1-4857-90b7-6ae01f52cfb1",
   "metadata": {},
   "source": [
    "## Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05b0af3-fa1b-4cf8-86e3-4eeaa10d4cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b8d4a3-3f3f-4b9d-8094-8a324337981f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## load document from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712bc270-3767-494b-8c10-90c3a3fe56d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load web doc via WebBaseLoader\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\",\"post-header\",\"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc500c9b-fac3-4a5f-a2b0-24758016e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29295\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0].page_content))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee97c1-e6fe-4539-9cc7-f95381b3d929",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Split doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099ff3df-cfdc-4a24-ba85-edae6430f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "## docs from above\n",
    "all_splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9024c09-3c18-4bbd-ac1e-533c360ea3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "103\n",
      "Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
      "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\n"
     ]
    }
   ],
   "source": [
    "# check chunks\n",
    "print(len(all_splits))\n",
    "print(len( all_splits[0].page_content ))\n",
    "print(all_splits[1].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8322dc-6b29-40d7-a78a-63ef672217f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'start_index': 114}\n"
     ]
    }
   ],
   "source": [
    "# print meta data \n",
    "\n",
    "print(all_splits[1].metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889bc95-17ce-46b8-bb78-2fb77aff6c89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding and Save\n",
    " - using Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4805910-da82-4e84-a414-90d7c89facbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# vector_store = Chroma(\n",
    "#     collection_name=\"lili_collection\",\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=\"./chroma_lili_collection_db\",\n",
    "# )\n",
    "\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59ef052c-a403-4e53-937c-2197f347882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_chroma.vectorstores.Chroma"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2b82cb-3206-46aa-ba36-1790bc0a8461",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Search Vector - Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c22e0a84-574d-4970-9528-14e63a4a0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6e0d13a-7639-431f-8197-5b4071502de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.vectorstores.base.VectorStoreRetriever"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5ef69c3-f9ad-4f85-980f-f38920b502f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs=retriever.invoke(\"What steps inclueds prompt engineering\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d218d0f7-7056-46ed-ab78-3bd062b6e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print( len(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4ddf677-36be-48af-9c53-c48455e1540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\n",
      "This post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\n"
     ]
    }
   ],
   "source": [
    "print( retrieved_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb7169-2b67-4e05-b564-b2ea230c8d5b",
   "metadata": {},
   "source": [
    "## Generate responds with GPT-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f81e8760-af89-4d9a-a960-4009d152fd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LLM \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a419718-0e99-4cdb-94d3-6d36a03285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure add env param LAINGCHAIN_API_KEY with value from LangSmith\n",
    "prompt=hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b87a4bd-655e-4b6f-a902-66ef4590c89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aaa77895-d256-45e7-a4cd-ea7e99fa4b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]\n"
     ]
    }
   ],
   "source": [
    "example_messages=prompt.invoke(\n",
    "    {\"context\":\"filler context\", \"question\":\"filler question\"}\n",
    ")\n",
    "print(example_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5ccd4ed-1d0a-4893-8168-a4385219b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc296108-dad9-489a-925b-54a73d0deeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the RAG Chain via pipes\n",
    "rag_chain=(\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3d1cf67-e83f-477f-8ac1-52e36b2d18b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering, also known as In-Context Prompting, involves methods to communicate with language models (LLMs) to achieve desired outcomes without altering the model's weights. It focuses on aligning and steering the model's behavior through carefully designed prompts, requiring extensive experimentation to find effective strategies. The goal is to enhance model responsiveness and alignment with user intentions."
     ]
    }
   ],
   "source": [
    "## Invok rag_chain\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Prompt Engineering?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b6c45-fa3b-420c-95a2-1ea8fe8d4c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f80024fe-244a-404e-86a4-d5f99ddf0c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## other prompt \n",
    "prompt = hub.pull(\"empler-ai/rag-prompt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14d91fd3-adde-4308-a9e4-9797cb8d8d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]\n"
     ]
    }
   ],
   "source": [
    "print(prompt.messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72eaa859-a981-4b44-8d1a-76305f097dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Engineering, also known as In-Context Prompting, refers to methods for communicating with large language models (LLMs) to steer their behavior towards desired outcomes without updating the model weights. It is an empirical science that often requires extensive experimentation and heuristics since the effectiveness of prompt engineering methods can vary significantly among different models. The primary goal of prompt engineering is to achieve alignment and model steerability, particularly for autoregressive language models."
     ]
    }
   ],
   "source": [
    "rag_chain=(\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What is Prompt Engineering?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a7d279-3bd4-4c7c-be1e-fc985f83bd91",
   "metadata": {},
   "source": [
    "## result compare \n",
    "\n",
    "### Prompt 1 \n",
    "\n",
    "- rlm/rag-prompt\n",
    "response:\n",
    "```\n",
    "Prompt Engineering, also known as In-Context Prompting, involves methods to communicate with language models (LLMs) to achieve desired outcomes without altering the model's weights. It focuses on aligning and steering the model's behavior through carefully designed prompts, requiring extensive experimentation to find effective strategies. The goal is to enhance model responsiveness and alignment with user intentions.\n",
    "```\n",
    "\n",
    "### Prompt 2\n",
    "\n",
    "- empler-ai/rag-prompt\n",
    "\n",
    "response:\n",
    "```\n",
    "Prompt Engineering, also known as In-Context Prompting, refers to methods for communicating with large language models (LLMs) to steer their behavior towards desired outcomes without updating the model weights. It is an empirical science that often requires extensive experimentation and heuristics since the effectiveness of prompt engineering methods can vary significantly among different models. The primary goal of prompt engineering is to achieve alignment and model steerability, particularly for autoregressive language models.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1734a8a2-ad21-4a44-b8db-f6c48f25049e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3b892-1bf7-4cd3-a59f-a74242197270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e04dd-cb3d-465a-8900-635255743bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849beac9-bce3-4016-91d8-e057429a49ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399039d7-f238-4012-8800-76798c8562c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2cd555-2a0c-49b5-9b05-ae7f7fe7f30e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe858f-0778-4217-a923-a37d76678798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ad51e-4047-404e-84e3-a9d2a682504a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
